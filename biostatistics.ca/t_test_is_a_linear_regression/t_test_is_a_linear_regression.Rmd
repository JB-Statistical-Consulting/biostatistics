---
title: T-Test Is A Linear Regression
date: \today
author:
output:
  html_document:
    css: "C:/Users/belai/Documents/JBConsulting/GitHub/biostatistics/biostatistics.ca/theme.css"
    #toc: TRUE
    #toc_float : TRUE
    #toc_depth : 2
    header_includes:
      - \usepackage{amsmath, amssymb, amsthm, mathtools}
      - \newcommand{\training}{\mathcal{X} \times \mathcal{Y}}
      - \newcommand{\defeq}{:=}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, class.source = "rcode", class.output = "rcode")
```

By Justin BÃ©lair &copy; | Biostatistician at [JB Statistical Consulting](www.justinbelair.ca)

[Find the RMarkdown Notebook on Github and Run the Code Yourself!](https://github.com/JB-Statistical-Consulting/biostatistics/tree/main/biostatistics.ca)

![](C:/Users/belai/Documents/JBConsulting/GitHub/biostatistics/biostatistics.ca/Justin-35(1).jpg){height=50%}

\newpage
# Hypothesis tests are based on underlying statistical models

I've heard before : "The data was not adequate for fitting a statistical model. Thus, only hypothesis tests were done." This does not make sense : 

1. A hypothesis test implies using a test statistic. 
2. A test statistic is then compared against the probability distribution we would expect if the null hypothesis were true. 
3. Having such a distribution to compare the test statistic against *implies* a statistical model of the data.

In short : no statistical inference without a model!

## The t-test example

We will do a very simple simulation experiment to understand how t-test and linear regression are related.

First, we simulate a binary vector specifying two groups, 0 and 1.

```{r}
set.seed(1) #for reproducibility

n <- 20 # sample size

groups <- rbinom(n, 1, 0.5) #Binary variable for groups with equal probability
groups
```

We simulate $y$ as a function of groups : observations in groups 1 will have a different mean. We also add some noise with `rnorm`. Here, this is a case where both groups, for example treatment vs placebo, would have different means.

```{r}
y <- 2*groups + rnorm(n,0,1)
y
```

### Testing the linear model coefficient for the `groups` variable...

We fit a linear model of $y$ as a function of `groups` and look at the model summary and the confidence interval for the regression parameter.

```{r}

lm.groups <- lm(y ~ groups)

#Notice the t-statistic of 3.407
summary(lm.groups)

#The two-sided 95% confidence interval for the regression parameter
confint(lm.groups, 'groups', level=0.95)
```

### ... is the same as running a t-test!

Alternatively, we separate the $y$ values in two groups and run a t-test^[We use `var.equal = T` because the default t-test in R (wisely) uses a correction for unequal variances.]

```{r t-test}
group0_values <- y[groups==0]
group1_values <- y[groups==1]


t.test(group1_values, group0_values,var.equal=T)

```

Notice the t-statistic and the 95% confidence interval are the same! 

This is not simply an artifact of how the data was prepared : we are literally estimating the same thing, the same way :)

This is true for many other hypothesis tests : do you know what statistical model is working under the hood of your favorite hypothesis test procedure?
